{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOnrInuUbOgjt21LP+D7A4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efwoods/tensorflow-mastery/blob/main/Predicting_Bitcoin_Prices_Naive_Bayes_vs_N_Beats_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3dh-dX9Y9lV"
      },
      "outputs": [],
      "source": [
        "# Historical Bitcoin Price\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3v4rCJa1ZxVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# read in bitcoin data and pass the dates\n",
        "# Parse the date and tell pandas that column 1 is a datetime\n",
        "df = pd.read_csv('/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv', parse_dates=[\"Date\"], index_col=[\"Date\"])"
      ],
      "metadata": {
        "id": "9mmHcO6CZJCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "16w1nEBzZx-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0RjFPmHJZ_Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many samples do we have?\n",
        "len(df)"
      ],
      "metadata": {
        "id": "LGHN9_3aaBxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()\n"
      ],
      "metadata": {
        "id": "NolfM5JcaF2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **NOTE** Seasonality is the number of samples per year."
      ],
      "metadata": {
        "id": "j0rHIy7PaHKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bitcoin_price = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})"
      ],
      "metadata": {
        "id": "FRBxA6V0aRfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bitcoin_price.head()"
      ],
      "metadata": {
        "id": "hW4nt3XKb-vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "bitcoin_price.plot(figsize=(10,7))\n",
        "plt.ylabel(\"Price\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.legend(fontsize=14)"
      ],
      "metadata": {
        "id": "uqPf9P2hcBaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing time series data with Python's CSV module"
      ],
      "metadata": {
        "id": "zthBy4FlcRGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "timesteps = []\n",
        "btc_price = []\n",
        "with open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", 'r') as f:\n",
        "  csv_reader = csv.reader(f, delimiter=',')\n",
        "  next(csv_reader) # This will skip the header\n",
        "  for line in csv_reader:\n",
        "    timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates\n",
        "    btc_price.append(float(line[2])) # get the closing price as a float"
      ],
      "metadata": {
        "id": "m0eked2Zc0gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps[:10], btc_price[:10]"
      ],
      "metadata": {
        "id": "hdCOJcUTeFOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(timesteps, btc_price)\n",
        "plt.ylabel(\"BTC Price\")\n",
        "plt.title(\"Price of bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\n",
        "plt.xlabel(\"Date\")"
      ],
      "metadata": {
        "id": "kVOBPuzaeJ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get bitcoin date array\n",
        "timesteps = bitcoin_price.index.to_numpy()\n",
        "prices = bitcoin_price[\"Price\"].to_numpy()"
      ],
      "metadata": {
        "id": "wYu7HP1dgbab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps[:10], prices[:10]"
      ],
      "metadata": {
        "id": "FuUvzshngxQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## The incorrect way to split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(timesteps,\n",
        "                                                    prices,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "bn7lrPeQg-S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(X_train, y_train, s=5, label=\"Train\")\n",
        "plt.scatter(X_test, y_test, s=5, label=\"Test data\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"BTC Price\")\n",
        "plt.legend(fontsize=14)"
      ],
      "metadata": {
        "id": "DbXNMqRmhoHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create train & test sets for time series"
      ],
      "metadata": {
        "id": "tjy-U8rJh09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_size = int(0.8 * len(prices))\n",
        "\n",
        "# Create test data splits (everything before the split)\n",
        "X_train, y_train = timesteps[:split_size], prices[:split_size]\n",
        "\n",
        "# Create test data splits (everything beyond the split)\n",
        "X_test, y_test = timesteps[split_size:], prices[split_size:]\n",
        "\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "I6FNQQboiSCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the correctly made splits\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(X_train, y_train, s=5, label=\"Train\")\n",
        "plt.scatter(X_test, y_test, s=5, label=\"Test\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"BTC Price\")\n",
        "plt.legend(fontsize=14)"
      ],
      "metadata": {
        "id": "6xXmYiAwi_sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a plotting function\n",
        "def plot_split_time_series(timesteps, values, format=\".\", start=0, end=None, label=None):\n",
        "  \"\"\"\n",
        "  Plot timesteps against values\n",
        "\n",
        "  Parameters\n",
        "  __________\n",
        "  timesteps : array of timestep values\n",
        "  values : array of values across time\n",
        "  format : style of plot, default \".\"\n",
        "  start : where to start the plot (setting a value will index from start of timesteps)\n",
        "  end : where to end the plot (similar to start but for the end)\n",
        "  label : label to show on plot about values, default None\n",
        "  \"\"\"\n",
        "  # Plot the series\n",
        "  plt.plot(timesteps[start:end], values[start:end], format, label=label)\n",
        "  plt.xlabel(\"Time\")\n",
        "  plt.ylabel(\"BTC Price\")\n",
        "  if label:\n",
        "    plt.legend(fontsize=14)\n",
        "  plt.grid(True)\n",
        "\n",
        "def plot_time_series(X_train, X_test, y_train, y_test, training_label=\"Train_data\", testing_label=\"Test data\"):\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plot_split_time_series(timesteps=X_train, values=y_train, label=training_label)\n",
        "  plot_split_time_series(timesteps=X_test, values=y_test, label=testing_label)\n",
        ""
      ],
      "metadata": {
        "id": "MEsUkpFbju4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out plotting function\n",
        "plot_time_series(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "FIP5AdeNla0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters:\n",
        "# Horizon: how far into the future we are predicting\n",
        "# Window size: number of steps used to predict the horizon"
      ],
      "metadata": {
        "id": "f-m_SycymQkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters:\n",
        "- **Horizon**: how far into the future we are predicting\n",
        "- **Window size**: number of steps used to predict the horizon\n"
      ],
      "metadata": {
        "id": "5p5NGBlXpCfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Naive forecast (baseline)\n",
        "\n",
        "### Formula\n",
        "\n",
        "$$\\hat{y}_{t} = y_{t-1}$$\n",
        "\n",
        "For a horizon of 1, the prediction @ timestep t (y-hat) is equal to the value at timestep t-1 (previous timestep)."
      ],
      "metadata": {
        "id": "QoIG6V6hpA01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a naive forecast\n",
        "naive_forecast = y_test[:-1]\n",
        "naive_forecast[:10], naive_forecast[-10:]"
      ],
      "metadata": {
        "id": "CrvLMuAlo-0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot naive forecast\n",
        "plt.figure(figsize=(10,7))\n",
        "# plot_split_time_series(X_train, y_train, label=\"Train data\")\n",
        "plot_split_time_series(X_test, y_test, start=350,  format=\"-\", label=\"Test data\")\n",
        "plot_split_time_series(X_test[1:], naive_forecast, start=350, format=\"-\", label=\"Naive Forecast\")"
      ],
      "metadata": {
        "id": "jZ-jDTvDqQcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "qpooNXkeqz0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MASE implementation\n",
        "def mean_absolute_scaled_error(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Implement mean absolute error (assuming no seasonality of the data).\n",
        "  \"\"\"\n",
        "  mae = tf.reduce_mean(tf.abs(y_true-y_pred))\n",
        "\n",
        "  # Find MAE of naive forecast (no seasonality)\n",
        "  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1]))\n",
        "\n",
        "  return mae / mae_naive_no_season\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "1bWxaar2tQkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_scaled_error(y_true=y_test[1:], y_pred=naive_forecast).numpy()"
      ],
      "metadata": {
        "id": "XhndXxOIt5U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate predictions\n",
        "# lower scores are better\n",
        "def evaluate_preds(y_true, y_pred):\n",
        "  # use float32 for calculations\n",
        "  # numpy uses float64; convert to float32\n",
        "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "  # Calculate the evaluation metrics\n",
        "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred) # Mean absolute error: on average, the prediction is off the forecast by this amount\n",
        "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
        "  rmse = tf.sqrt(mse)\n",
        "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "  return {\"mae\": mae.numpy(),\n",
        "          \"mse\": mse.numpy(),\n",
        "          \"rmse\": rmse.numpy(),\n",
        "          \"mape\": mape.numpy(),\n",
        "          \"mase\": mase.numpy()}\n",
        ""
      ],
      "metadata": {
        "id": "7AcHULJSuD_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_results = evaluate_preds(y_true=y_test[1:],\n",
        "                               y_pred=naive_forecast)\n",
        "naive_results"
      ],
      "metadata": {
        "id": "sg62QHgTvRlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Windowing the time series dataset\n",
        "# given a series (window) of data, predict the next price (horizon)\n",
        "\n",
        "# global variables for windows and horizon size\n",
        "HORIZON = 1 # pridict next 1 day\n",
        "WINDOW_SIZE = 7 # use the past week of Bitcoin data to make the prediction\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wbCD4HiFvc5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labelled_windows(x, horizon=HORIZON):\n",
        "  \"\"\"\n",
        "  Creates labels for windowed dataset.\n",
        "\n",
        "  E.G. if horizon is 1:\n",
        "  Input: [0,1,3,3,4,5,6,7] -> Output: ([0,1,2,3,4,5,6],[7])\n",
        "  \"\"\"\n",
        "  return x[:, :-horizon], x[:, -horizon]\n"
      ],
      "metadata": {
        "id": "6aZM7XkRxfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the window labelling function\n",
        "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0))\n",
        "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")"
      ],
      "metadata": {
        "id": "dZo4HflbyFgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.expand_dims(tf.range(8)+1, axis=0)"
      ],
      "metadata": {
        "id": "-XdGWyPzyPXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_windows(x, window_size=WINDOW_SIZE, horizon=HORIZON):\n",
        "  \"\"\"\n",
        "  Turns a 1D array into a 2D array of sequential labelled windows of window_size with horizon size labels.\n",
        "  \"\"\"\n",
        "  # 1. Create a window of specific window_size (add the horizon on the end for labelling later)\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "\n",
        "  # 2. Create a 2D array of multiple window steps (minus1 to account for 0 indexing)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2d array of windows of size window size)\n",
        "\n",
        "  print(f\"Window indexes: \\n {window_indexes, window_indexes.shape}\")\n",
        "\n",
        "  # 3. Index on the target array (a time series) with 2D array of multiple window steps\n",
        "  windowed_array = x[window_indexes]\n",
        "  # print(windowed_array)\n",
        "\n",
        "  # 4. Get labeled windows\n",
        "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "  return windows, labels\n"
      ],
      "metadata": {
        "id": "NDrmjJ9UyYoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices[:7]"
      ],
      "metadata": {
        "id": "w27d06a32AuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
        "len(full_windows), len(full_labels)"
      ],
      "metadata": {
        "id": "2UA1Xx7Z2Cpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(prices)\n"
      ],
      "metadata": {
        "id": "JWOD-5Rd2NVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 3 windows/labels\n",
        "for i in range(3):\n",
        "  print(f\"Window: {full_windows[i]} -> Label: {full_labels[i]}\")"
      ],
      "metadata": {
        "id": "WAHqlPPI3k41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Turning windows into training and test sets\n",
        "full_windows[:5], full_labels[:5]"
      ],
      "metadata": {
        "id": "mwCFVuzF3zBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# make train test splits\n",
        "def make_train_test_splits(windows, labels, test_split=0.2):\n",
        "  \"\"\"\n",
        "  Splits matching pairs of windows and labels into train and test splits.\n",
        "  \"\"\"\n",
        "  split_size = int(len(windows) * (1-test_split)) # this will default to the 80% 20% train / test split.\n",
        "  train_windows = windows[:split_size]\n",
        "  train_labels = labels[:split_size]\n",
        "  test_windows = windows[split_size:]\n",
        "  test_labels = labels[split_size:]\n",
        "  return train_windows, test_windows, train_labels, test_labels"
      ],
      "metadata": {
        "id": "8CGS5GW64iyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\n",
        "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "bAJvUZe25c78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_windows[:5], test_labels[:5]"
      ],
      "metadata": {
        "id": "4RVn41jB5sSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "R28hLWtx5wJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see if train labels are the same (before and after window split)\n",
        "np.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:])\n"
      ],
      "metadata": {
        "id": "YpepS1SM5yAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE A MODELING CHECKPOINT\n",
        "\n",
        "# Compare each of the best performance with another model's best performance\n",
        "# i.e. if the model's best performance is on epoch 55, but we are training to 100 epochs\n",
        "# then we will load and evaluate teh model saved on epoch 55.\n",
        "\n",
        "\n",
        "# Create a modeling checkpoint using the ModelCheckpoint callback from the tensorlfow api\n",
        "\n",
        "import os\n",
        "\n",
        "# Create a function to implement a ModelCheckpoint with a specific filename\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), verbose=0, save_best_only=True)"
      ],
      "metadata": {
        "id": "bnHej1dt6J7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "IQWUupKDFpTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters are values you adjust\n",
        "# Parameters values the model learns on its own\n",
        "\n",
        "# set random seed for reproducible results\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Construct model\n",
        "model_1 = tf.keras.Sequential([\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(HORIZON, activation='linear')\n",
        "], name=\"model_1_dense\")\n",
        "\n",
        "# 2. Compile the model\n",
        "model_1.compile(loss=\"mae\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"mae\",\"mse\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_1.fit(x=train_windows,\n",
        "            y=train_labels,\n",
        "            epochs=100,\n",
        "            verbose=1,\n",
        "            batch_size=128,\n",
        "            validation_data=(test_windows, test_labels),\n",
        "            callbacks=[create_model_checkpoint(model_name=model_1.name)])\n"
      ],
      "metadata": {
        "id": "Gihz7Cah8OIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data\n",
        "model_1.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "S2BYnEztGCLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in the saved best performing model_1 and evaluate it on test data\n",
        "model_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense/\")\n",
        "model_1.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "ZpmoRniSG_hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_results"
      ],
      "metadata": {
        "id": "XCk-AKyEHGyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## making a forecast on the dataset\n",
        "# 1. Take in a model\n",
        "# 2. Take in input data\n",
        "# 3. Passes the input data to the model's predict() method\n",
        "# 4. Returns the predictions\n",
        "\n",
        "def make_preds(model, input_data):\n",
        "  \"\"\"\n",
        "  Uses model to make predictions input_data.\n",
        "  \"\"\"\n",
        "  forecast = model.predict(input_data)\n",
        "  return tf.squeeze(forecast) # return 1D array of predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "33_NfTbMHbAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions using model_1\n",
        "model_1_preds = make_preds(model_1, test_windows)\n",
        "len(model_1_preds), model_1_preds[:10]\n"
      ],
      "metadata": {
        "id": "4cQTqBKjIc7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate preds\n",
        "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
        "                                 y_pred=model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "6aq89nnsIli2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the model 1 predictions\n",
        "offset = 300\n",
        "plt.figure(figsize=(10,7))\n",
        "plot_split_time_series(timesteps=X_test[-len(test_windows):],\n",
        "                       values=test_labels[:,],\n",
        "                       start=offset,\n",
        "                       label=\"Test Data\")\n",
        "\n",
        "plot_split_time_series(timesteps=X_test[-len(test_windows):],\n",
        "                       values=model_1_preds,\n",
        "                       start=offset,\n",
        "                       format=\"-\",\n",
        "                       label=\"Model 1 Predictions\")\n"
      ],
      "metadata": {
        "id": "Lq_wWFdhItQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 1\n",
        "WINDOW_SIZE = 30\n",
        "\n",
        "# Make window data with appropriate horizon and window sizes\n",
        "full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
        "len(full_windows), len(full_labels)\n"
      ],
      "metadata": {
        "id": "tM0mhE90JOCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revisit [Lecture 319 - 330](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/learn/lecture/27423946#overview)\n",
        "\n"
      ],
      "metadata": {
        "id": "FQDuAYETvji2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NBeats [Algorithm](https://arxiv.org/pdf/1905.10437.pdf)\n",
        "![figure 1 from N-BBEATS paper, the algorithm we're going to build](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-figure-1-nbeats-paper-annotated.png)"
      ],
      "metadata": {
        "id": "6kn83xKAvea8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 1\n",
        "WINDOW_SIZE = 7"
      ],
      "metadata": {
        "id": "gjdzQX780FsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create NBeatsBlock custom layer\n",
        "class NBeatsBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, # the constructor takes all the hyperparameters for the layer\n",
        "               input_size: int,\n",
        "               theta_size: int,\n",
        "               horizon: int,\n",
        "               n_neurons: int,\n",
        "               n_layers: int,\n",
        "               **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)\n",
        "    super().__init__(**kwargs)\n",
        "    self.input_size = input_size\n",
        "    self.theta_size = theta_size\n",
        "    self.horizon = horizon\n",
        "    self.n_neurons = n_neurons\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Block contains stack of 4 fully connected layers each has ReLU activation\n",
        "    self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n",
        "\n",
        "    # Output of block is a theta layer with linear activation\n",
        "    self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")\n",
        "\n",
        "  def call(self, inputs): # the call method is what runs when the layer is called\n",
        "    x = inputs\n",
        "    for layer in self.hidden: # pass inputs through each hidden layer\n",
        "      x = layer(x)\n",
        "    theta = self.theta_layer(x)\n",
        "    # Output the backcast and forecast from theata\n",
        "\n",
        "    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n",
        "\n",
        "    return backcast, forecast"
      ],
      "metadata": {
        "id": "NKhhMQsXvfOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up dummy NBeatsBlock layer to represent inputs and outputs\n",
        "tf.random.set_seed(42)\n",
        "dummy_nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE,\n",
        "                                       theta_size=WINDOW_SIZE+HORIZON,\n",
        "                                       horizon=HORIZON,\n",
        "                                       n_neurons=128,\n",
        "                                       n_layers=4)\n"
      ],
      "metadata": {
        "id": "8ynwDeyqxbS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy inputs (have to be same size as input_size)\n",
        "dummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE) + 1, axis=0) # input shape to the model has to reflect Dense layer input requirements (ndim=2\n",
        "dummy_inputs"
      ],
      "metadata": {
        "id": "_HgUg5rfx-oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass dummy inputs to dummy NBeatsBlock layer\n",
        "backcast, forecast = dummy_nbeats_block_layer(dummy_inputs)\n",
        "# These are the activation outpus of the theta layer (they'll be random due to no training of the model)\n",
        "print(f\"Backcast: {tf.squeeze(backcast.numpy())}\")\n",
        "print(f\"Forecast: {tf.squeeze(forecast.numpy())}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AERnkfWsyT5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preparing data for the NBeats Algorithm\n",
        "## Using a performant data pipeline\n",
        "\n",
        "bitcoin_price.head()"
      ],
      "metadata": {
        "id": "V8_jNgXfyoeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add windowed columns\n",
        "bitcoin_price_nbeats = bitcoin_price.copy()\n",
        "for i in range(WINDOW_SIZE):\n",
        "  bitcoin_price_nbeats[f\"Price+{i+1}\"] = bitcoin_price_nbeats[\"Price\"].shift(periods=i+1)\n",
        "bitcoin_price_nbeats.head()"
      ],
      "metadata": {
        "id": "16oKaHZNgIQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make features and labels\n",
        "X = bitcoin_price_nbeats.dropna().drop(\"Price\", axis=1)\n",
        "y = bitcoin_price_nbeats.dropna()[\"Price\"]\n",
        "\n",
        "# Make train and test sets\n",
        "split_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:split_size], y[:split_size]\n",
        "X_test, y_test = X[split_size:], y[split_size:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)\n"
      ],
      "metadata": {
        "id": "oibXYYlsglrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time to make our dataset performant using tf.data API\n",
        "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "\n",
        "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "\n",
        "# combine labels and features by zipping together -> (features, labels)\n",
        "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
        "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
        "\n",
        "# Batch and prefetch\n",
        "BATCH_SIZE = 1024\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "7dRyXEP9g89S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for the NBeats algorithm\n",
        "N_EPOCHS = 5000\n",
        "N_NEURONS = 512\n",
        "N_LAYERS = 4\n",
        "N_STACKS = 30\n",
        "\n",
        "INPUT_SIZE = WINDOW_SIZE * HORIZON\n",
        "THETA_SIZE = INPUT_SIZE + HORIZON\n",
        "\n",
        "INPUT_SIZE, THETA_SIZE\n"
      ],
      "metadata": {
        "id": "Nw53nKChiIL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make tensors\n",
        "tensor_1 = tf.range(10) + 10\n",
        "tensor_2 = tf.range(10)\n",
        "\n",
        "# Subtract\n",
        "subtracted = layers.subtract([tensor_1, tensor_2])\n",
        "\n",
        "# Add\n",
        "added = layers.add([tensor_1, tensor_2])\n",
        "\n",
        "# Get outputs\n",
        "print(f\"Input tensors: {tensor_1.numpy()} & {tensor_2.numpy()}\")\n",
        "print(f\"Subtracted: {subtracted.numpy()}\")\n",
        "print(f\"Added: {added.numpy()}\")"
      ],
      "metadata": {
        "id": "q2c05O0LjdyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Building, compiling, and fitting the N-BEATS algorithm\n",
        "%%time\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#1. setup an instance of the NBeatsBlock\n",
        "nbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,\n",
        "                                 theta_size=THETA_SIZE,\n",
        "                                 horizon=HORIZON,\n",
        "                                 n_neurons=N_NEURONS,\n",
        "                                 n_layers=N_LAYERS,\n",
        "                                 name=\"InitialBlock\")\n",
        "\n",
        "# 2. Create input to stack\n",
        "stack_input = layers.Input(shape=(INPUT_SIZE), name=\"stack_input\")\n",
        "\n",
        "# 3. Create initial backcast and forecast input (backwards prediction + horizon prediction)\n",
        "residuals, forecast = nbeats_block_layer(stack_input)\n",
        "\n",
        "# 4. Create stacks of block layers\n",
        "for i, _ in enumerate(range(N_STACKS-1)): # first stack is already created in step 3 above\n",
        "\n",
        "  # 5. Use the NBeatsBlock to calculate the backcast as well as the forecast\n",
        "  backcast, block_forecast = NBeatsBlock(\n",
        "    input_size=INPUT_SIZE,\n",
        "    theta_size=THETA_SIZE,\n",
        "    horizon=HORIZON,\n",
        "    n_neurons=N_NEURONS,\n",
        "    n_layers=N_LAYERS,\n",
        "    name=f\"NBeatsBlock_{i}\"\n",
        "  )(residuals) # pass in the residuals\n",
        "\n",
        "# 6. Create the double residual stacking\n",
        "residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\")\n",
        "forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n",
        "\n",
        "# 7. Put the stack model together\n",
        "model_7 = tf.keras.Model(inputs=stack_input, outputs=forecast, name=\"model_7_NBEATS\")\n",
        "\n",
        "# 8. Compile model with MAE loss\n",
        "model_7.compile(loss=\"mae\",optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
        "model_7.fit(train_dataset,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=test_dataset,\n",
        "            verbose=0,\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=200,restore_best_weights=True),tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",patience=100,verbose=1)])"
      ],
      "metadata": {
        "id": "Njk-qkPImSvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate N-Beatsl Model on the test dataset\n",
        "model_7.evaluate(test_dataset)\n",
        "\n",
        "# Make predictions with N-BEATS model\n",
        "model_7_preds = make_preds(model_7, test_dataset)\n",
        "model_7_preds[:10]\n",
        "\n",
        "# Evaluate N-BEATS model preds\n",
        "model_7_results = evaluate_preds(y_true=y_test,y_pred=model_7_preds)\n",
        "model_7_results"
      ],
      "metadata": {
        "id": "CfHRwMYXpohS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "id": "M3CUQyWfoiI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_results"
      ],
      "metadata": {
        "id": "XRq_vd4Vr0Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the N-BEATS architecture\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_7)"
      ],
      "metadata": {
        "id": "7v_Hh3gAr1zx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}